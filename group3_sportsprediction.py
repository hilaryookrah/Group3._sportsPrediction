# -*- coding: utf-8 -*-
"""Group3_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/113cPZya0syqC2KWisOSy9tDp7HfRsxJD
"""

#import libraries
import os
import sklearn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import tree, metrics
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from google.colab import drive
drive.mount('/content/drive')

sports_data_21=pd.read_csv('/content/drive/MyDrive/MIDSEM PROJECT/players_21.csv')#train

sports_data_21.info()

"""Drop features with 30% NaN"""

threshold = 0.30 * len(sports_data_21)

# Identify columns with 30% or more NaN values
columns_with_nan = sports_data_21.columns[sports_data_21.isna().sum() >= threshold]

# Drop the identified columns
sports_data_21.drop(columns=columns_with_nan, inplace=True)

"""Getting numerical and object columns"""

numerical_columns = sports_data_21.select_dtypes(include=['int64', 'float64'])

object_columns = sports_data_21.select_dtypes(include=['object']).columns

object_columns

"""Encoding object columns"""

for column in object_columns:
    sports_data_21[column] = pd.factorize(sports_data_21[column])[0]

"""Imputing  columns"""

# Create a SimpleImputer instance with your chosen strategy
imputer = SimpleImputer(strategy='mean')

# Iterate over columns and impute
for column in sports_data_21:
    sports_data_21[column] = imputer.fit_transform(sports_data_21[[column]])

sports_data_21.isna().any() #Check if there are any NaNs

c = sports_data_21.corr()
c["overall"].sort_values(ascending=False)

#Using correlation values greater than 0.4 we selected these
s =['overall','movement_reactions','passing','mentality_composure','dribbling','release_clause_eur','wage_eur','value_eur','power_shot_power','physic','mentality_vision',
           'attacking_short_passing','shooting','skill_long_passing','age','skill_ball_control','international_reputation','skill_curve','attacking_crossing']

#We picked the top 12
selected =['overall','movement_reactions','passing','dribbling','mentality_composure','power_shot_power','attacking_short_passing','shooting','age','physic','mentality_vision','international_reputation']

sports_data_21 = sports_data_21[selected]

sports_data_21.head()

"""Scaling"""

from sklearn.preprocessing import StandardScaler

y = sports_data_21['overall']

x =sports_data_21.drop('overall',axis=1)

X= StandardScaler()

scaled = X.fit_transform(x)

sports_data_21=pd.DataFrame(scaled,columns=x.columns)

sports_data_21

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error



"""Training Random Forest Regressor"""

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

rfm = RandomForestRegressor(n_estimators=110, random_state=45)
rfm.fit(X_train, y_train)

scores = cross_val_score(rfm, X_train, y_train, cv=10)
print("Cross-Validation Scores:", scores)

y_pred = rfm.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

mae = print(mean_absolute_error(y_test, y_pred))

"""Training with Decision Tree Classifier"""

from sklearn.tree import DecisionTreeRegressor

dtree = DecisionTreeRegressor(max_depth=15, min_samples_split=10)

dtree.fit(X_train,y_train)

y_pred = dtree.predict(X_test)

scores = cross_val_score(dtree, X_train, y_train, cv=10)
print("Cross-Validation Scores:", scores)

mae = print(mean_absolute_error(y_test, y_pred))

from sklearn.metrics import accuracy_score,f1_score,make_scorer

"""Training using Gradient Boost

"""

from sklearn.ensemble import GradientBoostingRegressor

grad=GradientBoostingRegressor( n_estimators=100, learning_rate=0.1)

grad.fit(X_train,y_train)

y_pred=grad.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

scores = cross_val_score(grad, X_train, y_train, cv=10)
print("Cross-Validation Scores:", scores)

mae = print(mean_absolute_error(y_test, y_pred))

"""Testing with KNeighborsClassifier and SVC"""

m_ae = print(mean_absolute_error(y_test, y_pred))

"""Voting Regressor"""

from sklearn.ensemble import VotingRegressor

# Create a VotingRegressor instance
voting_regressor = VotingRegressor(
    estimators=[
        ('gradboost', grad),
        ('rf', rfm),
        ('decision tree', dtree)
    ],
#  n_jobs=-1,  # Parallelize the training if available
#     weights=[1, 1, 1]  # Use 'hard' or 'soft' for aggregation
)

for model in (grad, rfm,dtree,voting_regressor):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate Mean Absolute Error (MAE)
    mae = mean_absolute_error(y_test, y_pred)

    # Print the model's name and MAE
    print(model.__class__.__name__, "MAE:", mae)

"""Optimization Of Models"""

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold

cv = KFold(n_splits=6)
# Define the parameter grid
PARAMETERS = {
    "max_depth": [2, 5, 6, 12],
    "min_samples_split": [2, 5, 7,10],
    "n_estimators": [100,120, 150]
}

def custom_scorer(y_true, y_pred):
    return -mean_absolute_error(y_true, y_pred)  # Using negative MAE for GridSearchCV

rf_regressor = RandomForestRegressor(n_jobs=-1)
#Define the scoring function for regression (e.g., mean squared error)
scoring = make_scorer(mean_squared_error, greater_is_better=False)

# Create a GridSearchCV instance
model_gs = GridSearchCV(
    rf_regressor,
    param_grid=PARAMETERS,
    cv=cv,
    scoring=scoring,
    verbose=0
)

# Fit the model to the training data
model_gs.fit(X_train, y_train)

# Make predictions
y_pred = model_gs.predict(X_test)

"""Testing with Players 22"""

sports_data_22=pd.read_csv('/content/drive/MyDrive/MIDSEM PROJECT/players_22.csv')#train

threshold = 0.30 * len(sports_data_22)

# Identify columns with 30% or more NaN values
columns_with_nan = sports_data_22.columns[sports_data_22.isna().sum() >= threshold]

# Drop the identified columns
sports_data_22.drop(columns=columns_with_nan, inplace=True)

num_columns = sports_data_22.select_dtypes(include=['int64', 'float64'])

obj_columns = sports_data_22.select_dtypes(include=['object']).columns

for column in obj_columns:
    sports_data_22[column] = pd.factorize(sports_data_22[column])[0]

# Create a SimpleImputer instance with your chosen strategy
imputer = SimpleImputer(strategy='mean')

# Iterate over columns and impute
for column in sports_data_22:
    sports_data_22[column] = imputer.fit_transform(sports_data_22[[column]])

#We picked the top 12
selected_data =['overall','movement_reactions','passing','dribbling','mentality_composure','power_shot_power','attacking_short_passing','shooting','age','physic','mentality_vision','international_reputation']

sports_data_22 = sports_data_22[selected_data]

y_new= sports_data_22['overall']

x_new = sports_data_22.drop('overall',axis=1)

sc= StandardScaler()

scaled_new = sc.fit_transform(x_new)

sports_data_22=pd.DataFrame(scaled_new,columns=x_new.columns)

"""Testing with random forest regressor"""

# Now you can make predictions on the imputed data
y_pred_new = rfm.predict(x_new)

maeee = print(mean_absolute_error(y_new, y_pred_new))

"""Testing with Decision Tree Regressor"""

# Now you can make predictions on the imputed data
y_pred_new = dtree.predict(x_new)

maee = print(mean_absolute_error(y_new, y_pred_new))

"""Testing with Gradient Boost Regressor"""

# Now you can make predictions on the imputed data
y_pred_new = grad.predict(x_new)

maeee = print(mean_absolute_error(y_new, y_pred_new))

"""Saving the Random Forest Regressor"""

import pickle
filename = 'finalized_best_model.pkl'
pickle.dump(rfm, open(filename, 'wb'))
# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
y_pred = loaded_model.predict(X_test)
print(y_pred)

# import pickle

# # Assuming you have a trained model named 'model'
# best_model_path = '/content/my_model.pkl'  # Specify the file path where you want to save the model

# # Save the model to a file
# with open(best_model_path, 'wb') as file:
#     pickle.dump(rfm, file)

# from google.colab import files

# # Download the saved model
# files.download(best_model_path)

